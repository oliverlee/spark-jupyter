{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left\" src=\"images/spark.png\">\n",
    "<img style=\"float: right\" src=\"images/surfsara.png\">\n",
    "<hr style=\"clear: both\">\n",
    "\n",
    "## 03 - The 20 newsgroups data\n",
    "\n",
    "Below are number of exercises in Python and Pyspark. Press Shift-Enter to execute the code. You can use code completion by using tab.\n",
    "\n",
    "In this notebook we will introduce the dataset we will use today and apply some Spark transformations. Finally, we will store preprocessed data that can be used in later notebooks.\n",
    "\n",
    " 1. The data\n",
    " 2. 'Munging' the data and counting words (again...)\n",
    " 3. Dataframes to the rescue\n",
    " 4. Storing preprocessed data for later use\n",
    "\n",
    "During the exercises you may want to refer to [The PySpark documentation](https://spark.apache.org/docs/latest/api/python/pyspark.html) for more information on possible transformations and actions.\n",
    "\n",
    "First initialize Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize Spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "if not 'sc' in globals():\n",
    "    conf = SparkConf().setMaster('local[*]')\n",
    "    sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data: 20 newsgroups\n",
    "\n",
    "For the exercises today we will use the 20 newsgroups data. This datasets contains around 20000 newsgroup postings from 20 different newsgroups. A description of the original dataset can be found [here](http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.data.html). \n",
    "\n",
    "We have already converted the raw dataset into a more manageable format: each newsgroup posting has been converted to a JSON object and these objects have been stored together in one large gzipped file (only some common headers were preserved in this process). This file is available in your notebook environment. And can be easily loaded using the SparkContext: sc. Load the data as [text](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=textFile#pyspark.SparkContext) file and print the first element of the RDD (use [first](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.first)). Note that you do not need to worry about decompressing the file first and can read straight from the gzipped file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"date\":\"Mon, 29 Mar 1993 11:57:19 GMT\",\"summary\":\"Books, addresses, music -- anything related to atheism\",\"in-reply-to\":\"\",\"followup-to\":\"alt.atheism\",\"xref\":\"cantaloupe.srv.cs.cmu.edu alt.atheism:49960 alt.atheism.moderated:713 news.answers:7054 alt.answers:126\",\"newsgroups\":\"alt.atheism,alt.atheism.moderated,news.answers,alt.answers\",\"references\":\"\",\"keywords\":\"FAQ, atheism, books, music, fiction, addresses, contacts\",\"nntp-posting-host\":\"\",\"subject\":\"Alt.Atheism FAQ: Atheist Resources\",\"article-id\":\"\",\"originator\":\"\",\"label\":\"alt.atheism\",\"distribution\":\"world\",\"x-newsreader\":\"\",\"content\":\"\\nArchive-name: atheism/resources\\nAlt-atheism-archive-name: resources\\nLast-modified: 11 December 1992\\nVersion: 1.0\\n\\n                              Atheist Resources\\n\\n                      Addresses of Atheist Organizations\\n\\n                                     USA\\n\\nFREEDOM FROM RELIGION FOUNDATION\\n\\nDarwin fish bumper stickers and assorted other atheist paraphernalia are\\navailable from the Freedom From Religion Foundation in the US.\\n\\nWrite to:  FFRF, P.O. Box 750, Madison, WI 53701.\\nTelephone: (608) 256-8900\\n\\nEVOLUTION DESIGNS\\n\\nEvolution Designs sell the \\\"Darwin fish\\\".  It's a fish symbol, like the ones\\nChristians stick on their cars, but with feet and the word \\\"Darwin\\\" written\\ninside.  The deluxe moulded 3D plastic fish is $4.95 postpaid in the US.\\n\\nWrite to:  Evolution Designs, 7119 Laurel Canyon #4, North Hollywood,\\n           CA 91605.\\n\\nPeople in the San Francisco Bay area can get Darwin Fish from Lynn Gold --\\ntry mailing <figmo@netcom.com>.  For net people who go to Lynn directly, the\\nprice is $4.95 per fish.\\n\\nAMERICAN ATHEIST PRESS\\n\\nAAP publish various atheist books -- critiques of the Bible, lists of\\nBiblical contradictions, and so on.  One such book is:\\n\\n\\\"The Bible Handbook\\\" by W.P. Ball and G.W. Foote.  American Atheist Press.\\n372 pp.  ISBN 0-910309-26-4, 2nd edition, 1986.  Bible contradictions,\\nabsurdities, atrocities, immoralities... contains Ball, Foote: \\\"The Bible\\nContradicts Itself\\\", AAP.  Based on the King James version of the Bible.\\n\\nWrite to:  American Atheist Press, P.O. Box 140195, Austin, TX 78714-0195.\\n      or:  7215 Cameron Road, Austin, TX 78752-2973.\\nTelephone: (512) 458-1244\\nFax:       (512) 467-9525\\n\\nPROMETHEUS BOOKS\\n\\nSell books including Haught's \\\"Holy Horrors\\\" (see below).\\n\\nWrite to:  700 East Amherst Street, Buffalo, New York 14215.\\nTelephone: (716) 837-2475.\\n\\nAn alternate address (which may be newer or older) is:\\nPrometheus Books, 59 Glenn Drive, Buffalo, NY 14228-2197.\\n\\nAFRICAN-AMERICANS FOR HUMANISM\\n\\nAn organization promoting black secular humanism and uncovering the history of\\nblack freethought.  They publish a quarterly newsletter, AAH EXAMINER.\\n\\nWrite to:  Norm R. Allen, Jr., African Americans for Humanism, P.O. Box 664,\\n           Buffalo, NY 14226.\\n\\n                                United Kingdom\\n\\nRationalist Press Association          National Secular Society\\n88 Islington High Street               702 Holloway Road\\nLondon N1 8EW                          London N19 3NL\\n071 226 7251                           071 272 1266\\n\\nBritish Humanist Association           South Place Ethical Society\\n14 Lamb's Conduit Passage              Conway Hall\\nLondon WC1R 4RH                        Red Lion Square\\n071 430 0908                           London WC1R 4RL\\nfax 071 430 1271                       071 831 7723\\n\\nThe National Secular Society publish \\\"The Freethinker\\\", a monthly magazine\\nfounded in 1881.\\n\\n                                   Germany\\n\\nIBKA e.V.\\nInternationaler Bund der Konfessionslosen und Atheisten\\nPostfach 880, D-1000 Berlin 41. Germany.\\n\\nIBKA publish a journal:\\nMIZ. (Materialien und Informationen zur Zeit. Politisches\\nJournal der Konfessionslosesn und Atheisten. Hrsg. IBKA e.V.)\\nMIZ-Vertrieb, Postfach 880, D-1000 Berlin 41. Germany.\\n\\nFor atheist books, write to:\\n\\nIBDK, Internationaler B\\\"ucherdienst der Konfessionslosen\\nPostfach 3005, D-3000 Hannover 1. Germany.\\nTelephone: 0511/211216\\n\\n\\n                               Books -- Fiction\\n\\nTHOMAS M. DISCH\\n\\n\\\"The Santa Claus Compromise\\\"\\nShort story.  The ultimate proof that Santa exists.  All characters and \\nevents are fictitious.  Any similarity to living or dead gods -- uh, well...\\n\\nWALTER M. MILLER, JR\\n\\n\\\"A Canticle for Leibowitz\\\"\\nOne gem in this post atomic doomsday novel is the monks who spent their lives\\ncopying blueprints from \\\"Saint Leibowitz\\\", filling the sheets of paper with\\nink and leaving white lines and letters.\\n\\nEDGAR PANGBORN\\n\\n\\\"Davy\\\"\\nPost atomic doomsday novel set in clerical states.  The church, for example,\\nforbids that anyone \\\"produce, describe or use any substance containing...\\natoms\\\". \\n\\nPHILIP K. DICK\\n\\nPhilip K. Dick Dick wrote many philosophical and thought-provoking short \\nstories and novels.  His stories are bizarre at times, but very approachable.\\nHe wrote mainly SF, but he wrote about people, truth and religion rather than\\ntechnology.  Although he often believed that he had met some sort of God, he\\nremained sceptical.  Amongst his novels, the following are of some relevance:\\n\\n\\\"Galactic Pot-Healer\\\"\\nA fallible alien deity summons a group of Earth craftsmen and women to a\\nremote planet to raise a giant cathedral from beneath the oceans.  When the\\ndeity begins to demand faith from the earthers, pot-healer Joe Fernwright is\\nunable to comply.  A polished, ironic and amusing novel.\\n\\n\\\"A Maze of Death\\\"\\nNoteworthy for its description of a technology-based religion.\\n\\n\\\"VALIS\\\"\\nThe schizophrenic hero searches for the hidden mysteries of Gnostic\\nChristianity after reality is fired into his brain by a pink laser beam of\\nunknown but possibly divine origin.  He is accompanied by his dogmatic and\\ndismissively atheist friend and assorted other odd characters.\\n\\n\\\"The Divine Invasion\\\"\\nGod invades Earth by making a young woman pregnant as she returns from\\nanother star system.  Unfortunately she is terminally ill, and must be\\nassisted by a dead man whose brain is wired to 24-hour easy listening music.\\n\\nMARGARET ATWOOD\\n\\n\\\"The Handmaid's Tale\\\"\\nA story based on the premise that the US Congress is mysteriously\\nassassinated, and fundamentalists quickly take charge of the nation to set it\\n\\\"right\\\" again.  The book is the diary of a woman's life as she tries to live\\nunder the new Christian theocracy.  Women's right to own property is revoked,\\nand their bank accounts are closed; sinful luxuries are outlawed, and the\\nradio is only used for readings from the Bible.  Crimes are punished\\nretroactively: doctors who performed legal abortions in the \\\"old world\\\" are\\nhunted down and hanged.  Atwood's writing style is difficult to get used to\\nat first, but the tale grows more and more chilling as it goes on.\\n\\nVARIOUS AUTHORS\\n\\n\\\"The Bible\\\"\\nThis somewhat dull and rambling work has often been criticized.  However, it\\nis probably worth reading, if only so that you'll know what all the fuss is\\nabout.  It exists in many different versions, so make sure you get the one\\ntrue version.\\n\\n                             Books -- Non-fiction\\n\\nPETER DE ROSA\\n\\n\\\"Vicars of Christ\\\", Bantam Press, 1988\\nAlthough de Rosa seems to be Christian or even Catholic this is a very\\nenlighting history of papal immoralities, adulteries, fallacies etc.\\n(German translation: \\\"Gottes erste Diener. Die dunkle Seite des Papsttums\\\",\\nDroemer-Knaur, 1989)\\n\\nMICHAEL MARTIN\\n\\n\\\"Atheism: A Philosophical Justification\\\", Temple University Press,\\n Philadelphia, USA.\\nA detailed and scholarly justification of atheism.  Contains an outstanding\\nappendix defining terminology and usage in this (necessarily) tendentious\\narea.  Argues both for \\\"negative atheism\\\" (i.e. the \\\"non-belief in the\\nexistence of god(s)\\\") and also for \\\"positive atheism\\\" (\\\"the belief in the\\nnon-existence of god(s)\\\").  Includes great refutations of the most\\nchallenging arguments for god; particular attention is paid to refuting\\ncontempory theists such as Platinga and Swinburne.\\n541 pages. ISBN 0-87722-642-3 (hardcover; paperback also available)\\n\\n\\\"The Case Against Christianity\\\", Temple University Press\\nA comprehensive critique of Christianity, in which he considers\\nthe best contemporary defences of Christianity and (ultimately)\\ndemonstrates that they are unsupportable and/or incoherent.\\n273 pages. ISBN 0-87722-767-5\\n\\nJAMES TURNER\\n\\n\\\"Without God, Without Creed\\\", The Johns Hopkins University Press, Baltimore,\\n MD, USA\\nSubtitled \\\"The Origins of Unbelief in America\\\".  Examines the way in which\\nunbelief (whether agnostic or atheistic)  became a mainstream alternative\\nworld-view.  Focusses on the period 1770-1900, and while considering France\\nand Britain the emphasis is on American, and particularly New England\\ndevelopments.  \\\"Neither a religious history of secularization or atheism,\\nWithout God, Without Creed is, rather, the intellectual history of the fate\\nof a single idea, the belief that God exists.\\\" \\n316 pages. ISBN (hardcover) 0-8018-2494-X (paper) 0-8018-3407-4\\n\\nGEORGE SELDES (Editor)\\n\\n\\\"The great thoughts\\\", Ballantine Books, New York, USA\\nA \\\"dictionary of quotations\\\" of a different kind, concentrating on statements\\nand writings which, explicitly or implicitly, present the person's philosophy\\nand world-view.  Includes obscure (and often suppressed) opinions from many\\npeople.  For some popular observations, traces the way in which various\\npeople expressed and twisted the idea over the centuries.  Quite a number of\\nthe quotations are derived from Cardiff's \\\"What Great Men Think of Religion\\\"\\nand Noyes' \\\"Views of Religion\\\".\\n490 pages. ISBN (paper) 0-345-29887-X.\\n\\nRICHARD SWINBURNE\\n\\n\\\"The Existence of God (Revised Edition)\\\", Clarendon Paperbacks, Oxford\\nThis book is the second volume in a trilogy that began with \\\"The Coherence of\\nTheism\\\" (1977) and was concluded with \\\"Faith and Reason\\\" (1981).  In this\\nwork, Swinburne attempts to construct a series of inductive arguments for the\\nexistence of God.  His arguments, which are somewhat tendentious and rely\\nupon the imputation of late 20th century western Christian values and\\naesthetics to a God which is supposedly as simple as can be conceived, were\\ndecisively rejected in Mackie's \\\"The Miracle of Theism\\\".  In the revised\\nedition of \\\"The Existence of God\\\", Swinburne includes an Appendix in which he\\nmakes a somewhat incoherent attempt to rebut Mackie.\\n\\nJ. L. MACKIE\\n\\n\\\"The Miracle of Theism\\\", Oxford\\nThis (posthumous) volume contains a comprehensive review of the principal\\narguments for and against the existence of God.  It ranges from the classical\\nphilosophical positions of Descartes, Anselm, Berkeley, Hume et al, through\\nthe moral arguments of Newman, Kant and Sidgwick, to the recent restatements\\nof the classical theses by Plantinga and Swinburne.  It also addresses those\\npositions which push the concept of God beyond the realm of the rational,\\nsuch as those of Kierkegaard, Kung and Philips, as well as \\\"replacements for\\nGod\\\" such as Lelie's axiarchism.  The book is a delight to read - less\\nformalistic and better written than Martin's works, and refreshingly direct\\nwhen compared with the hand-waving of Swinburne.\\n\\nJAMES A. HAUGHT\\n\\n\\\"Holy Horrors: An Illustrated History of Religious Murder and Madness\\\",\\n Prometheus Books\\nLooks at religious persecution from ancient times to the present day -- and\\nnot only by Christians.\\nLibrary of Congress Catalog Card Number 89-64079. 1990.\\n\\nNORM R. ALLEN, JR.\\n\\n\\\"African American Humanism: an Anthology\\\"\\nSee the listing for African Americans for Humanism above.\\n\\nGORDON STEIN\\n\\n\\\"An Anthology of Atheism and Rationalism\\\", Prometheus Books\\nAn anthology covering a wide range of subjects, including 'The Devil, Evil\\nand Morality' and 'The History of Freethought'.  Comprehensive bibliography.\\n\\nEDMUND D. COHEN\\n\\n\\\"The Mind of The Bible-Believer\\\", Prometheus Books\\nA study of why people become Christian fundamentalists, and what effect it\\nhas on them.\\n\\n                                Net Resources\\n\\nThere's a small mail-based archive server at mantis.co.uk which carries\\narchives of old alt.atheism.moderated articles and assorted other files.  For\\nmore information, send mail to archive-server@mantis.co.uk saying\\n\\n   help\\n   send atheism/index\\n\\nand it will mail back a reply.\\n\\n\\nmathew\\nï¿½\\n\",\"path\":\"cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!bb3.andrew.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!magnus.acs.ohio-state.edu!usenet.ins.cwru.edu!agate!spool.mu.edu!uunet!pipex!ibmpcug!mantis!mathew\",\"approved\":\"news-answers-request@mit.edu\",\"sender\":\"\",\"organization\":\"Mantis Consultants, Cambridge. UK.\",\"message-id\":\"<19930329115719@mantis.co.uk>\",\"from\":\"mathew <mathew@mantis.co.uk>\",\"lines\":\"290\",\"reply-to\":\"\"}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "jsonPostsRDD = sc.textFile(\"file://///home/jovyan/work/data/20newsgroups.labelled.json.gz\")\n",
    "\n",
    "# Print the first element of the RDD\n",
    "print jsonPostsRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If all went well you should have seen that the first element is a JSON document. In the next few exercises we will use RDDs and transformations to first convert the json data to a more manageable format. Then we will select one newsgroup, count the words and find the top 20 senders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data conversion\n",
    "\n",
    "Next, we are going to convert the elements into JSON format. This will return a dictionary where each key is a property of the JSON doc. The newsgroups posts are in a 'flat' JSON object, so there are no embedded JSON object. \n",
    "\n",
    "In the next cell the conversion takes place and the first post is shown. \n",
    "\n",
    "Just execute the cell, there's noting to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "parsedJsonPostsRDD = jsonPostsRDD.map(lambda x: json.loads(x))\n",
    "parsedPosts = parsedJsonPostsRDD.take(1)\n",
    "print json.dumps(parsedPosts, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access to fields in the posts\n",
    "\n",
    "We have made some selections for you to show how to access fields in the posts.\n",
    "\n",
    "This is pure Python (using dictionaries), although the data is contained in an RDD. You probably see what's going on here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "postSelectionRDD = parsedJsonPostsRDD.map(lambda d: [d[\"label\"], d[\"sender\"], d[\"date\"], d[\"subject\"]])\n",
    "print postSelectionRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Spark it is important to keep track of what code is executed on workers, and what code on the driver. To move data to and from the driver to the workers is very expensive Given a large enough datasets - it might not even be possible to move the contents of an RDD to the driver.\n",
    "\n",
    "RDDs are distributed over workers and transformations define a sequence of RDDs. Never try to define an RDD inside an RDD and beware of what code is executed by the driver.\n",
    "\n",
    "Let's make a quick list of all properties in a post. We'll do it the wrong way first, by doing a map on the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print parsedJsonPostsRDD.map(lambda x: x.keys()).take(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code is very inefficient, since all elements in the RDD are processed, and we end up with an RDD with all keys for all posts. It would be better to take a single post and then outside an RDD compute the keys. Note that then the computation of the keys is done by the driver.\n",
    "\n",
    "Try to do this in a single statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "print parsedJsonPostsRDD<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting properties, filtering on those properties, counting words and senders/posters\n",
    "\n",
    "In the next few exercises we will make a selection of a few properties, filter out a specific newsgroup and count the words and top 10 posters (senders).\n",
    "\n",
    "Start by creating an RDD with the following headers or properties : {label, from, subject, content}. Print the first element of the resulting RDD to verify your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "postsRDD = parsedJsonPostsRDD.map(<FILL IN>)\n",
    "print postsRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering an RDD\n",
    "\n",
    "Besides map and reduce Spark has many convenient transformations on RDDs available out-of-the-box. In the next exercise use [filter](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.filter) to create an RDD that only consists of postings to the 'sci.space' newsgroup (do onsider the datatype - an array per post -  in the RDD you created in the previous exercise). Print the number of postings in the newsgroup using [count](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.count) and print the first post in the RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "scispacePostsRDD = postsRDD.filter(<FILL IN>)\n",
    "print scispacePostsRDD.count()\n",
    "print scispacePostsRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting words\n",
    "\n",
    "To do a wordcount on the newsgroup we first need to do a bit of cleaning up.\n",
    "\n",
    "Use a series of map operations to transform the content part of the RDD from the previous exercise. Instead of the content as a long string we want to end up with the content part as a list of tokens or words. Before creating that list encode the data as utf-8 and replace all non-words with single space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "tokenizedPostsRDD = (scispacePostsRDD<FILL IN>)\n",
    "print tokenizedPostsRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will count the words in the newsgroup by again applying several transformations on the tokenizedPostsRDD:\n",
    "\n",
    "1. use a flatMap to create an RDD that consists of only the words from the posts\n",
    "2. use a filter to remove all words with a length shorter than 2 characters\n",
    "3. create (word, 1) tuples, lowercase each word\n",
    "4. use reduceByKey  to add the results for each word\n",
    "\n",
    "We will print the top ten words by making use of the [takeOrdered](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=takeordered#pyspark.RDD.takeOrdered) action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "wcRDD = (tokenizedPostsRDD<FILL IN>)\n",
    "print wcRDD.takeOrdered(10, lambda x : -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting senders/posters\n",
    "\n",
    "Using similar operations as in the wordcount exercise, give a top-ten list of posters using the 'from' property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "fromCountRDD = (scispacePostsRDD<FILL IN>)\n",
    "print fromCountRDD.takeOrdered(10, lambda x : -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### DataFrames to the rescue\n",
    "\n",
    "As you can see from the previous exercises transforming and converting data can be done quite easily using regular RDDs and transformations. There are however even more intuitive ways to deal with your data - provided it is in a structured format (i.e. it has a schema).\n",
    "\n",
    "From version Spark 1.0.0 onwards SparkSQL has been available in the Spark distribution. The SparkSQL module offers functions for loading and reading structured data, SQL syntax on the resulting loaded data and offers data organized in [DataFrames](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame). \n",
    "\n",
    "In the following steps we will load the JSON newsgroup data the SparkSQL way and explore the capabilities of this API.\n",
    "\n",
    "First, create an [SQLContext](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext) from the SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [SQLContext](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext) provides access to a [DataFrameReader](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader) which can directory read from several structured formats or connectiions such as JSON, [Parquet](https://parquet.apache.org), JDBC and Hive.\n",
    "\n",
    "Load the json into a DataFrame using the SQLContext and print the schema of the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame = sqlCtx.read.json(\"file://///home/jovyan/work/data/20newsgroups.labelled.json.gz\")\n",
    "dataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame API provides a domain-specific language, alongside SQL, for manipulating structured data. It is, among others, possible to filter, select and group data.\n",
    "\n",
    "Grouping by the sender column and counting them is trivial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfGroups = dataFrame.groupBy(\"sender\").count()\n",
    "dfGroups.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering and selecting columns is equally simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfFilter = dataFrame.filter(dataFrame.label == \"sci.space\")\n",
    "dfSelect = dfFilter.select(\"label\", \"from\", \"subject\")\n",
    "dfSelect.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are more comfortable with SQL the SparkSQL module allows you to use this as well. \n",
    "\n",
    "In order to talk SQL to a DataFrame it is needed to associate the DataFrame with a table name to use in SQL statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataFrame.registerTempTable('newsgroups')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use SQL statements to query the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = sqlCtx.sql(\"SELECT label, content FROM newsgroups\")\n",
    "labelsDistinct = sqlCtx.sql(\"SELECT DISTINCT(label) FROM newsgroups\")\n",
    "print sqlCtx.sql(\"SELECT COUNT(DISTINCT(label)) as labelCount FROM newsgroups\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a list of the different newsgroups in the dataset, and broadcast this list to all executors so that it can be used in expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelList = labelsDistinct.map(lambda r: r.label.encode('utf-8')).collect()\n",
    "sc.broadcast(labelList)\n",
    "from IPython.display import display, HTML\n",
    "th = \"<th>Label</th>\"\n",
    "td = [\"<tr><td>\" + d + \"</td></tr>\" for d in labelList]\n",
    "display(HTML(\"<table><thead><tr>\" + \"\".join(th) + \"</tr></thead><tbody>\" + \"\".join(td) + \"</tbody></table>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed 20 newsgroups. For machine learning it is probably convenient to have a numerical ID or label associated with each group. Let's create this ID and show a table of the mapping and the number of posts per newsgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = sqlCtx.sql(\"SELECT label FROM newsgroups\")\n",
    "labelCounts = labels.map(lambda r: (str(r.label), 1)).reduceByKey(lambda v1, v2: v1 + v2).collect()\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "th = \"<th>ID</th><th>Label</th><th>Messages</th>\"\n",
    "td = [\"<tr><td>\" + str(labelList.index(l)) +\"</td><td>\" + l + \"</td><td>\" + str(m) +\"</tr>\" for (l,m) in labelCounts]\n",
    "display(HTML(\"<table><thead><tr>\" + \"\".join(th) + \"</tr></thead><tbody>\" + \"\".join(td) + \"</tbody></table>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing data for later use\n",
    "\n",
    "In the final steps of this notebook we will make a subset of the data that is ready to use for subsequent notebooks;\n",
    "\n",
    "1. select only the following groups: {2,3,5,6,8,11,12,13,18,19}\n",
    "2. select only the following columns: {id, label, content}\n",
    "3. clean and tokenize the content column\n",
    "4. Safe the resulting dataframe into a Parquet file\n",
    "\n",
    "First select the relevant groups and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selectedGroups = sqlCtx.sql(\"SELECT label, content FROM newsgroups WHERE label in ('sci.med', 'rec.autos', 'comp.windows.x', 'rec.sport.baseball', 'misc.forsale', 'talk.politics.misc', 'comp.graphics', 'alt.atheism', 'sci.space', 'talk.religion.misc')\")\n",
    "selectedGroups.show(5)\n",
    "print selectedGroups.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DataFrame can be accessed as an RDD of [Rows](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row), although it is not simply a wrapper around RDD's. The way functions are applied to data using DataFrames differs from plain lambda's on RDDs; this becomes evident when working with functions such as the withColumn (this has to be a special kind of function: registered UDF's to be used in SQL).\n",
    "\n",
    "Here we will use this pattern to create a new RDD with an added column - the id. After adding the column we will chain a few extra maps to tokenize and clean the content field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "selectedGroupsWithID = (selectedGroups.withColumn(\"id\", selectedGroups.label)\n",
    "                        .map(lambda r: (r.label.encode(\"utf-8\"), re.sub('\\s+', ' ', r.content).strip(), labelList.index(r.label)))\n",
    "                        .map(lambda (l,c,i): (l, c.encode('utf-8').lower(), i))\n",
    "                        .map(lambda (l,c,i): (l, c.split(\" \"), i))\n",
    "                        )\n",
    "                                                     \n",
    "print selectedGroupsWithID.first()\n",
    "\n",
    "selectionDF = sqlCtx.createDataFrame(selectedGroupsWithID, [\"label\", \"content\", \"id\"])\n",
    "print \"\\n\"\n",
    "selectionDF.printSchema()\n",
    "print \"\\n\"\n",
    "selectionDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally, we will store the dataframe in an efficient format. For the columnar data in our dataframe [Parquet](https://parquet.apache.org) seems a good match. Fortunately the DataFrame API comes with [batteries included](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.parquet) and storing our data is a one-liner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selectionDF.write.parquet(\"file://///home/jovyan/work/data/20newsgroups.selected.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
